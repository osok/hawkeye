#!/usr/bin/env python3
"""
AI-Powered Threat Analysis Demonstration

This script demonstrates the new AI-powered dynamic threat analysis system
for MCP servers, showing how it replaces the static hardcoded approach.
"""

import os
import sys
import json
from pathlib import Path
import time # Added for streaming demonstrations

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from hawkeye.detection.ai_threat import AIThreatAnalyzer, MCPCapabilityAnalyzer
from hawkeye.detection.ai_threat.models import (
    EnvironmentContext, DeploymentType, SecurityPosture, DataSensitivity,
    NetworkExposure, UserPrivileges, ComplianceFramework
)
from hawkeye.detection.mcp_introspection.models import MCPServerInfo, MCPTool
from hawkeye.config import get_settings


def create_sample_mcp_server() -> MCPServerInfo:
    """Create a sample MCP server for demonstration."""
    
    # Sample tools that would be discovered
    sample_tools = [
        MCPTool(
            name="read_file",
            description="Read contents of a file from the filesystem",
            input_schema={
                "type": "object", 
                "properties": {
                    "path": {"type": "string", "description": "File path to read"}
                },
                "required": ["path"]
            }
        ),
        MCPTool(
            name="web_search", 
            description="Search the web for information",
            input_schema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Search query"},
                    "max_results": {"type": "integer", "description": "Maximum results"}
                },
                "required": ["query"]
            }
        ),
        MCPTool(
            name="execute_command",
            description="Execute a system command",
            input_schema={
                "type": "object",
                "properties": {
                    "command": {"type": "string", "description": "Command to execute"}
                },
                "required": ["command"]
            }
        )
    ]
    
    # Import the capability model
    from hawkeye.detection.mcp_introspection.models import MCPCapability
    
    # Create sample capabilities as a list
    sample_capabilities = [
        MCPCapability(
            name="tools",
            description="Tool management capabilities", 
            capabilities=["listChanged"],
            metadata={"version": "1.0"}
        ),
        MCPCapability(
            name="resources",
            description="Resource management capabilities",
            capabilities=["subscribe", "listChanged"], 
            metadata={"version": "1.0"}
        )
    ]
    
    return MCPServerInfo(
        server_id="filesystem-and-web-mcp-12345",
        server_url="stdio://npx/@modelcontextprotocol/server-filesystem-and-web",
        tools=sample_tools,
        capabilities=sample_capabilities,
        metadata={
            "name": "filesystem-and-web-mcp",
            "server_type": "npm",
            "transport_type": "stdio",
            "host": "localhost",
            "port": None,
            "command_line": "npx @modelcontextprotocol/server-filesystem-and-web /tmp",
            "process_id": 12345,
            "confidence": 0.95
        }
    )


def demonstrate_capability_analysis():
    """Demonstrate the capability analysis component."""
    print("ğŸ” CAPABILITY ANALYSIS DEMONSTRATION")
    print("=" * 50)
    
    # Create sample MCP server
    mcp_server = create_sample_mcp_server()
    
    # Analyze capabilities
    analyzer = MCPCapabilityAnalyzer()
    capabilities = analyzer.analyze_tool(mcp_server)
    
    print(f"Tool Name: {capabilities.tool_name}")
    print(f"Tool ID: {capabilities.tool_id}")
    print(f"Functions Found: {len(capabilities.tool_functions)}")
    
    for func in capabilities.tool_functions:
        print(f"  - {func.name}: {func.description[:60]}...")
        print(f"    Categories: {[c.value for c in func.categories]}")
        print(f"    Risk Indicators: {func.risk_indicators}")
        print(f"    Requires Privileges: {func.requires_privileges}")
        print(f"    External Access: {func.external_access}")
    
    print(f"\nCapability Categories: {[c.value for c in capabilities.capability_categories]}")
    print(f"Risk Score: {capabilities.risk_surface.risk_score:.2f}")
    print(f"Confidence: {capabilities.confidence:.2f}")
    print()


def demonstrate_ai_analysis():
    """Demonstrate the AI-powered threat analysis."""
    print("ğŸ¤– AI-POWERED THREAT ANALYSIS DEMONSTRATION")
    print("=" * 50)
    
    try:
        # Create sample MCP server
        mcp_server = create_sample_mcp_server()
        
        # Create environment context
        env_context = EnvironmentContext(
            deployment_type=DeploymentType.LOCAL,
            security_posture=SecurityPosture.MEDIUM,
            data_sensitivity=DataSensitivity.INTERNAL,
            network_exposure=NetworkExposure.INTERNAL,
            user_privileges=UserPrivileges.STANDARD,
            compliance_requirements=[ComplianceFramework.OWASP_TOP_10]
        )
        
        # Initialize AI threat analyzer
        analyzer = AIThreatAnalyzer()
        
        print(f"Using AI Provider: {get_settings().ai.provider}")
        print(f"Model: {getattr(get_settings().ai, f'{get_settings().ai.provider}_model')}")
        
        # Check if API keys are configured
        ai_settings = get_settings().ai
        has_openai = bool(ai_settings.openai_api_key)
        has_anthropic = bool(ai_settings.anthropic_api_key)
        
        if not has_openai and not has_anthropic:
            print("\nâš ï¸  No AI API keys configured!")
            print("To test AI analysis, set environment variables:")
            print("  export AI_OPENAI_API_KEY='your-openai-key'")
            print("  export AI_ANTHROPIC_API_KEY='your-anthropic-key'")
            print("\nFalling back to rule-based analysis...\n")
        
        # Perform threat analysis
        print("Analyzing threats...")
        threat_analysis = analyzer.analyze_threats(mcp_server, env_context)
        
        # Display results
        print(f"\nğŸ“Š THREAT ANALYSIS RESULTS")
        print(f"Tool: {threat_analysis.tool_capabilities.tool_name}")
        print(f"Threat Level: {threat_analysis.threat_level.value.upper()}")
        print(f"Confidence: {threat_analysis.confidence_score:.2f}")
        print(f"AI Provider: {threat_analysis.analysis_metadata.provider}")
        print(f"Analysis Cost: ${threat_analysis.analysis_metadata.cost:.4f}")
        
        print(f"\nğŸ¯ ATTACK VECTORS ({len(threat_analysis.attack_vectors)} found):")
        for i, vector in enumerate(threat_analysis.attack_vectors[:3], 1):  # Show first 3
            print(f"{i}. {vector.name} (Severity: {vector.severity.value})")
            print(f"   Description: {vector.description}")
            print(f"   Impact: {vector.impact}")
            print(f"   Likelihood: {vector.likelihood:.2f}")
            if vector.prerequisites:
                print(f"   Prerequisites: {', '.join(vector.prerequisites)}")
            print()
        
        print(f"ğŸ›¡ï¸  MITIGATION STRATEGIES ({len(threat_analysis.mitigation_strategies)} found):")
        for i, strategy in enumerate(threat_analysis.mitigation_strategies[:2], 1):  # Show first 2
            print(f"{i}. {strategy.name}")
            print(f"   Description: {strategy.description}")
            print(f"   Effectiveness: {strategy.effectiveness_score:.2f}")
            if strategy.implementation_steps:
                print(f"   Implementation Steps:")
                for step in strategy.implementation_steps[:3]:  # Show first 3 steps
                    print(f"     - {step}")
            print()
        
        # Show statistics
        stats = analyzer.get_analysis_stats()
        print(f"ğŸ“ˆ ANALYSIS STATISTICS:")
        print(f"  Analyses Performed: {stats['analyses_performed']}")
        print(f"  Cache Hits: {stats['cache_hits']}")
        print(f"  Total Cost: ${stats['total_cost']:.4f}")
        print()
        
    except Exception as e:
        print(f"âŒ AI Analysis Error: {e}")
        print("This is expected if AI API keys are not configured.")
        print()


def demonstrate_batch_analysis():
    """Demonstrate batch analysis of multiple MCP servers."""
    print("ğŸ“š BATCH ANALYSIS DEMONSTRATION")
    print("=" * 50)
    
    # Import the capability model
    from hawkeye.detection.mcp_introspection.models import MCPCapability
    
    # Create multiple sample servers
    servers = [
        create_sample_mcp_server(),
        # Add a second server with different capabilities
        MCPServerInfo(
            server_id="database-mcp-23456",
            server_url="http://localhost:8080",
            tools=[
                MCPTool(
                    name="query_database",
                    description="Execute SQL queries on the database",
                    input_schema={
                        "type": "object",
                        "properties": {
                            "sql": {"type": "string", "description": "SQL query to execute"}
                        },
                        "required": ["sql"]
                    }
                )
            ],
            capabilities=[
                MCPCapability(
                    name="database",
                    description="Database access capabilities",
                    capabilities=["query", "transaction"],
                    metadata={"database_type": "postgresql"}
                )
            ],
            metadata={
                "name": "database-mcp",
                "server_type": "docker",
                "transport_type": "http",
                "host": "localhost",
                "port": 8080,
                "command_line": "docker run database-mcp",
                "process_id": 23456,
                "confidence": 0.88
            }
        )
    ]
    
    try:
        analyzer = AIThreatAnalyzer()
        
        print(f"Analyzing {len(servers)} MCP servers...")
        analyses = analyzer.analyze_multiple_threats(servers)
        
        print(f"\nğŸ“Š BATCH ANALYSIS RESULTS:")
        print(f"Successfully analyzed: {len(analyses)}/{len(servers)} servers")
        
        for analysis in analyses:
            print(f"\nâ€¢ {analysis.tool_capabilities.tool_name}")
            print(f"  Threat Level: {analysis.threat_level.value}")
            print(f"  Attack Vectors: {len(analysis.attack_vectors)}")
            print(f"  Mitigations: {len(analysis.mitigation_strategies)}")
            print(f"  Confidence: {analysis.confidence_score:.2f}")
        
        # Show final statistics
        stats = analyzer.get_analysis_stats()
        print(f"\nğŸ“ˆ FINAL STATISTICS:")
        print(f"  Total Analyses: {stats['analyses_performed']}")
        print(f"  Cache Efficiency: {stats['cache_hits']}/{stats['cache_hits'] + stats['cache_misses']} hits")
        print(f"  Total Cost: ${stats['total_cost']:.4f}")
        
    except Exception as e:
        print(f"âŒ Batch Analysis Error: {e}")
    
    print()


def show_configuration():
    """Show current AI configuration."""
    print("âš™ï¸  CONFIGURATION")
    print("=" * 50)
    
    settings = get_settings()
    ai_config = settings.ai
    
    print(f"Primary Provider: {ai_config.provider}")
    print(f"Fallback Provider: {ai_config.fallback_provider}")
    print(f"Cache Enabled: {ai_config.cache_enabled}")
    print(f"Cache TTL: {ai_config.cache_ttl} seconds")
    print(f"Max Cost per Analysis: ${ai_config.max_cost_per_analysis}")
    print(f"Debug Logging: {ai_config.debug_logging}")
    
    print(f"\nOpenAI Configuration:")
    print(f"  Model: {ai_config.openai_model}")
    print(f"  Max Tokens: {ai_config.openai_max_tokens}")
    print(f"  Temperature: {ai_config.openai_temperature}")
    print(f"  API Key Configured: {'âœ…' if ai_config.openai_api_key else 'âŒ'}")
    
    print(f"\nAnthropic Configuration:")
    print(f"  Model: {ai_config.anthropic_model}")
    print(f"  Max Tokens: {ai_config.anthropic_max_tokens}")
    print(f"  Temperature: {ai_config.anthropic_temperature}")
    print(f"  API Key Configured: {'âœ…' if ai_config.anthropic_api_key else 'âŒ'}")
    
    print(f"\nLocal LLM Configuration:")
    print(f"  Endpoint: {ai_config.local_llm_endpoint}")
    print(f"  Model: {ai_config.local_llm_model}")
    print(f"  Timeout: {ai_config.local_llm_timeout} seconds")
    print()


def demonstrate_parallel_analysis():
    """Demonstrate the parallel processing capabilities of the AIThreatAnalyzer."""
    print("ğŸš€ PARALLEL ANALYSIS DEMONSTRATION")
    print("=" * 50)
    
    try:
        # Create sample MCP server
        mcp_server = create_sample_mcp_server()
        
        # Create environment context
        env_context = EnvironmentContext(
            deployment_type=DeploymentType.LOCAL,
            security_posture=SecurityPosture.MEDIUM,
            data_sensitivity=DataSensitivity.INTERNAL,
            network_exposure=NetworkExposure.INTERNAL,
            user_privileges=UserPrivileges.STANDARD,
            compliance_requirements=[ComplianceFramework.OWASP_TOP_10]
        )
        
        # Initialize AI threat analyzer
        analyzer = AIThreatAnalyzer()
        
        print(f"Using AI Provider: {get_settings().ai.provider}")
        print(f"Model: {getattr(get_settings().ai, f'{get_settings().ai.provider}_model')}")
        
        # Check if API keys are configured
        ai_settings = get_settings().ai
        has_openai = bool(ai_settings.openai_api_key)
        has_anthropic = bool(ai_settings.anthropic_api_key)
        
        if not has_openai and not has_anthropic:
            print("\nâš ï¸  No AI API keys configured!")
            print("To test AI analysis, set environment variables:")
            print("  export AI_OPENAI_API_KEY='your-openai-key'")
            print("  export AI_ANTHROPIC_API_KEY='your-anthropic-key'")
            print("\nFalling back to rule-based analysis...\n")
        
        # Create multiple MCP servers for parallel demonstration
        mcp_servers = [mcp_server]
        
        # Create a second sample server for parallel processing demo
        mcp_server2 = MCPServerInfo(
            server_id="filesystem-server-2",
            name="Enhanced Filesystem Server",
            version="2.1.0",
            host="localhost",
            port=3001,
            is_secure=True,
            has_authentication=True,
            detected_via="process_enum",
            transport_type="stdio",
            server_type="nodejs",
            metadata={
                "name": "Enhanced Filesystem Server",
                "version": "2.1.0",
                "capabilities": ["filesystem", "network"],
                "tools": ["write_file", "delete_file", "http_request"],
                "resources": ["local_files", "remote_data"]
            }
        )
        mcp_servers.append(mcp_server2)
        
        # Perform parallel threat analysis
        print(f"Analyzing {len(mcp_servers)} MCP servers in parallel...")
        
        def progress_callback(completed, total, current_tool):
            print(f"  Progress: {completed}/{total} - Processing: {current_tool}")
        
        result = analyzer.analyze_threats_parallel(
            mcp_servers=mcp_servers,
            environment_context=env_context,
            analysis_type="comprehensive",
            max_workers=2,
            progress_callback=progress_callback
        )
        
        # Display parallel analysis results
        successful_analyses = result["successful_analyses"]
        errors = result["errors"]
        statistics = result["statistics"]
        
        print(f"\nğŸ“Š PARALLEL ANALYSIS RESULTS")
        print(f"âœ… Successful Analyses: {len(successful_analyses)}")
        print(f"âŒ Errors: {len(errors)}")
        print(f"â±ï¸  Total Time: {statistics['execution_time']:.2f}s")
        print(f"âš¡ Tools per Second: {statistics['tools_per_second']:.2f}")
        print(f"ğŸ”„ Success Rate: {statistics['success_rate']:.1%}")
        print(f"ğŸ‘¥ Workers Used: {statistics['max_workers']}")
        print(f"ğŸ“ˆ Parallel Efficiency: {statistics['parallel_efficiency']:.2f}")
        
        # Display results for each successful analysis
        for tool_name, threat_analysis in successful_analyses.items():
            print(f"\nğŸ” ANALYSIS: {tool_name}")
            print(f"  Threat Level: {threat_analysis.threat_level.value.upper()}")
            print(f"  Confidence: {threat_analysis.confidence_score:.2f}")
            print(f"  Attack Vectors: {len(threat_analysis.attack_vectors)}")
            print(f"  Mitigations: {len(threat_analysis.mitigation_strategies)}")
            print(f"  Provider: {threat_analysis.analysis_metadata.provider}")
            print(f"  Cost: ${threat_analysis.analysis_metadata.cost:.4f}")
        
        # Show any errors
        if errors:
            print(f"\nâš ï¸  ERRORS ENCOUNTERED:")
            for tool_name, error in errors.items():
                print(f"  {tool_name}: {error}")
        
        # Compare with sequential analysis
        print(f"\nâš¡ PERFORMANCE COMPARISON:")
        print(f"  Parallel Time: {statistics['execution_time']:.2f}s")
        estimated_sequential = statistics['avg_time_per_tool'] * statistics['total_tools']
        print(f"  Estimated Sequential: {estimated_sequential:.2f}s")
        if estimated_sequential > 0:
            speedup = estimated_sequential / statistics['execution_time']
            print(f"  Speedup: {speedup:.2f}x faster")
        print()
        
    except Exception as e:
        print(f"âŒ Parallel Analysis Error: {e}")
        print("This is expected if AI API keys are not configured.")
        print()


def demonstrate_streaming_analysis():
    """
    Demonstrate the new streaming analysis capabilities (F4.2).
    """
    print("\n" + "ğŸš€" + "="*78)
    print("ğŸš€ STREAMING ANALYSIS DEMONSTRATION (Phase 4: F4.2)")
    print("ğŸš€" + "="*78)
    
    try:
        # Initialize analyzer
        analyzer = AIThreatAnalyzer()
        
        # Detect MCP servers
        print("\nğŸ” DETECTING MCP SERVERS...")
        mcp_servers = [create_sample_mcp_server()] # Use a single sample server for streaming demo
        
        if not mcp_servers:
            print("âŒ No MCP servers detected for streaming demo")
            return
        
        print(f"âœ… Detected {len(mcp_servers)} MCP servers for streaming analysis")
        
        # Demonstrate basic streaming
        print(f"\nğŸ“¡ STREAMING ANALYSIS - Real-time Results")
        print("-" * 60)
        
        streaming_start = time.time()
        successful_count = 0
        error_count = 0
        
        # Stream results in real-time
        for event in analyzer.analyze_threats_streaming(
            mcp_servers=mcp_servers,
            analysis_type="comprehensive",
            max_workers=2
        ):
            event_type = event.get("event_type")
            timestamp = event.get("timestamp", "")
            
            if event_type == "initialization":
                print(f"ğŸ¬ [INIT] Starting analysis of {event['total_tools']} tools "
                      f"with {event['max_workers']} workers")
                      
            elif event_type == "result":
                successful_count += 1
                tool_name = event["tool_name"]
                progress = f"{event['completed']}/{event['total']}"
                print(f"âœ… [RESULT] {tool_name} - Analysis complete ({progress})")
                
                # Show key threat insights
                analysis = event.get("analysis")
                if analysis:
                    threat_level = getattr(analysis, 'overall_threat_level', 'Unknown')
                    attack_vectors = getattr(analysis, 'attack_vectors', [])
                    print(f"   ğŸ¯ Threat Level: {threat_level} | Vectors: {len(attack_vectors)}")
                
            elif event_type == "error":
                error_count += 1
                tool_name = event["tool_name"]
                error_msg = event["error"]
                progress = f"{event['completed']}/{event['total']}"
                print(f"âŒ [ERROR] {tool_name} - {error_msg[:50]}... ({progress})")
                
            elif event_type == "progress":
                progress_percent = event["progress_percent"]
                completed = event["completed"]
                total = event["total"]
                print(f"ğŸ“Š [PROGRESS] {completed}/{total} tools ({progress_percent:.1f}%)")
                
            elif event_type == "completion":
                streaming_time = time.time() - streaming_start
                statistics = event.get("statistics", {})
                
                print(f"\nğŸ [COMPLETE] Streaming analysis finished!")
                print(f"   âœ… Successful: {len(event['successful_analyses'])}")
                print(f"   âŒ Errors: {len(event['errors'])}")
                print(f"   â±ï¸  Total Time: {streaming_time:.2f}s")
                
                # Show performance metrics
                if statistics:
                    parallel_efficiency = statistics.get("parallel_efficiency", 0)
                    avg_time = statistics.get("average_analysis_time", 0)
                    print(f"   ğŸš€ Parallel Efficiency: {parallel_efficiency:.1f}")
                    print(f"   âš¡ Avg Analysis Time: {avg_time:.2f}s")
                    
            elif event_type == "execution_error":
                print(f"ğŸ’¥ [EXEC_ERROR] {event['error']}")
        
        print(f"\nğŸ“ˆ STREAMING ANALYSIS SUMMARY")
        print(f"   â€¢ Real-time result streaming as analyses complete")
        print(f"   â€¢ Immediate feedback on {successful_count + error_count} tools")
        print(f"   â€¢ Zero wait time for first results")
        print(f"   â€¢ Stream processing efficiency demonstrated")
        
    except Exception as e:
        print(f"âŒ Streaming analysis demo failed: {e}")
        import traceback
        traceback.print_exc()

def demonstrate_single_tool_streaming():
    """
    Demonstrate detailed single tool streaming with stage-by-stage progress.
    """
    print("\n" + "ğŸ”¬" + "="*78)
    print("ğŸ”¬ SINGLE TOOL STREAMING DEMONSTRATION (Detailed Stages)")
    print("ğŸ”¬" + "="*78)
    
    try:
        # Initialize analyzer
        analyzer = AIThreatAnalyzer()
        
        # Get one MCP server for detailed analysis
        mcp_servers = [create_sample_mcp_server()] # Use a single sample server for streaming demo
        if not mcp_servers:
            print("âŒ No MCP servers detected for single tool streaming demo")
            return
        
        target_server = mcp_servers[0]  # Use first server
        tool_name = target_server.metadata.get('name', target_server.server_id)
        
        print(f"ğŸ¯ Target Tool: {tool_name}")
        print(f"ğŸ“¡ Streaming detailed analysis stages...")
        print("-" * 60)
        
        # Stream single tool analysis with detailed stages
        for event in analyzer.analyze_single_tool_streaming(
            mcp_server=target_server,
            analysis_type="comprehensive"
        ):
            event_type = event.get("event_type")
            
            if event_type == "tool_start":
                print(f"ğŸ¬ [START] Beginning analysis of {event['tool_name']}")
                
            elif event_type == "stage_start":
                stage = event["stage"]
                stage_num = event["stage_number"]
                total_stages = event["total_stages"]
                print(f"ğŸ”„ [STAGE {stage_num}/{total_stages}] Starting {stage}...")
                
            elif event_type == "stage_complete":
                stage = event["stage"]
                stage_num = event["stage_number"]
                print(f"âœ… [STAGE {stage_num}] Completed {stage}")
                
                # Show stage-specific details
                if stage == "capability_analysis":
                    print(f"   ğŸ” Capabilities extracted and categorized")
                elif stage == "context_building":
                    print(f"   ğŸŒ Environment context established")
                elif stage == "ai_analysis":
                    print(f"   ğŸ¤– AI threat analysis completed")
                elif stage == "attack_chain_analysis":
                    print(f"   â›“ï¸  Attack chain patterns identified")
                
            elif event_type == "tool_complete":
                execution_time = event["execution_time"]
                print(f"\nğŸ [COMPLETE] Tool analysis finished in {execution_time:.2f}s")
                
                # Show analysis summary
                analysis = event.get("analysis")
                if analysis:
                    threat_level = getattr(analysis, 'overall_threat_level', 'Unknown')
                    attack_vectors = getattr(analysis, 'attack_vectors', [])
                    mitigations = getattr(analysis, 'mitigation_strategies', [])
                    
                    print(f"   ğŸ¯ Overall Threat Level: {threat_level}")
                    print(f"   âš”ï¸  Attack Vectors Found: {len(attack_vectors)}")
                    print(f"   ğŸ›¡ï¸  Mitigation Strategies: {len(mitigations)}")
                    
            elif event_type == "tool_error":
                error_msg = event["error"]
                print(f"âŒ [ERROR] Analysis failed: {error_msg}")
        
        print(f"\nğŸ”¬ SINGLE TOOL STREAMING SUMMARY")
        print(f"   â€¢ Stage-by-stage progress visibility")
        print(f"   â€¢ Real-time feedback on analysis components")
        print(f"   â€¢ Detailed breakdown of analysis workflow")
        print(f"   â€¢ Immediate error detection and reporting")
        
    except Exception as e:
        print(f"âŒ Single tool streaming demo failed: {e}")
        import traceback
        traceback.print_exc()

def demonstrate_batch_streaming():
    """
    Demonstrate batch streaming that combines resource management with real-time results.
    """
    print("\n" + "ğŸ“¦" + "="*78)
    print("ğŸ“¦ BATCH STREAMING DEMONSTRATION (Resource Management + Streaming)")
    print("ğŸ“¦" + "="*78)
    
    try:
        # Initialize analyzer
        analyzer = AIThreatAnalyzer()
        
        # Detect MCP servers
        mcp_servers = [create_sample_mcp_server()] # Use a single sample server for batch streaming demo
        if not mcp_servers:
            print("âŒ No MCP servers detected for batch streaming demo")
            return
        
        batch_size = min(2, len(mcp_servers))  # Small batches for demo
        print(f"ğŸ¯ Processing {len(mcp_servers)} tools in batches of {batch_size}")
        print(f"ğŸ“¡ Streaming results with batch resource management...")
        print("-" * 60)
        
        # Stream batch analysis
        for event in analyzer.analyze_threats_batch_streaming(
            mcp_servers=mcp_servers,
            batch_size=batch_size,
            delay_between_batches=0.5,
            analysis_type="comprehensive"
        ):
            event_type = event.get("event_type")
            
            if event_type == "initialization":
                print(f"ğŸ¬ [INIT] Starting batch streaming: {event['total_batches']} "
                      f"batches of {event['batch_size']} tools each")
                      
            elif event_type == "batch_start":
                batch_num = event["batch_number"]
                total_batches = event["total_batches"]
                batch_size = event["batch_size"]
                print(f"\nğŸ“¦ [BATCH {batch_num}/{total_batches}] Starting batch ({batch_size} tools)")
                
            elif event_type == "result":
                tool_name = event["tool_name"]
                batch_num = event.get("batch_number", "?")
                batch_progress = event.get("batch_progress", 0)
                batch_total = event.get("batch_total", 0)
                print(f"   âœ… [B{batch_num}] {tool_name} - Complete ({batch_progress}/{batch_total})")
                
            elif event_type == "error":
                tool_name = event["tool_name"]
                batch_num = event.get("batch_number", "?")
                batch_progress = event.get("batch_progress", 0)
                batch_total = event.get("batch_total", 0)
                print(f"   âŒ [B{batch_num}] {tool_name} - Error ({batch_progress}/{batch_total})")
                
            elif event_type == "batch_complete":
                batch_num = event["batch_number"]
                successful = event["batch_successful"]
                errors = event["batch_errors"]
                batch_time = event["batch_time"]
                print(f"   ğŸ [B{batch_num}] Batch complete: {successful} success, "
                      f"{errors} errors in {batch_time:.2f}s")
                
            elif event_type == "batch_delay":
                delay = event["delay_seconds"]
                print(f"   â¸ï¸  [DELAY] Waiting {delay}s before next batch...")
                
            elif event_type == "completion":
                total_time = event["total_time"]
                statistics = event.get("statistics", {})
                
                print(f"\nğŸ [COMPLETE] All batches finished in {total_time:.2f}s")
                print(f"   âœ… Successful: {len(event['successful_analyses'])}")
                print(f"   âŒ Errors: {len(event['errors'])}")
                
                # Show batch statistics
                if statistics.get("batch_stats"):
                    avg_batch_time = sum(b["batch_time"] for b in statistics["batch_stats"]) / len(statistics["batch_stats"])
                    print(f"   ğŸ“Š Average Batch Time: {avg_batch_time:.2f}s")
        
        print(f"\nğŸ“¦ BATCH STREAMING SUMMARY")
        print(f"   â€¢ Controlled resource usage with batching")
        print(f"   â€¢ Real-time results within each batch")
        print(f"   â€¢ Rate limiting between batches")
        print(f"   â€¢ Optimal balance of performance and control")
        
    except Exception as e:
        print(f"âŒ Batch streaming demo failed: {e}")
        import traceback
        traceback.print_exc()

def main():
    """Main demonstration function."""
    print("ğŸ¦… HAWKEYE AI-POWERED THREAT ANALYSIS DEMO")
    print("=" * 60)
    print("This demo showcases the new AI-powered dynamic threat analysis")
    print("system that replaces the static hardcoded approach.\n")
    
    # Show configuration
    show_configuration()
    
    # Run demonstrations
    demonstrate_capability_analysis()
    demonstrate_ai_analysis()
    demonstrate_batch_analysis()
    demonstrate_parallel_analysis()  # NEW: Demonstrate parallel processing
    
    # NEW: Streaming demonstrations (F4.2)
    demonstrate_streaming_analysis()
    demonstrate_single_tool_streaming()  
    demonstrate_batch_streaming()
    
    # === F4.3 OPTIMIZED BATCH PROCESSING DEMONSTRATION ===
    print("\n" + "ğŸš€" + "="*78)
    print("ğŸš€ F4.3: OPTIMIZED BATCH PROCESSING DEMONSTRATION")
    print("ğŸš€" + "="*78)
    print("\nğŸ¯ This demonstration showcases advanced batch processing optimization:")
    print("   â€¢ Adaptive batch sizing based on performance metrics")
    print("   â€¢ Intelligent load balancing across AI providers")  
    print("   â€¢ Memory usage optimization and monitoring")
    print("   â€¢ Smart prioritization strategies")
    print("   â€¢ Advanced performance tracking and analytics")
    
    # Demonstrate optimized batch processing
    print("\nğŸ“Š Running optimized batch analysis with adaptive sizing...")
    try:
        start_time = time.time()
        
        # Configure optimized batch parameters
        batch_config = {
            "adaptive_sizing": True,
            "target_batch_time": 15.0,  # Target 15 seconds per batch
            "min_batch_size": 1,
            "max_batch_size": 3,
            "enable_load_balancing": True,
            "memory_limit_mb": 256,
            "priority_strategy": "complexity"
        }
        
        print(f"ğŸ”§ Batch Configuration:")
        print(f"   â€¢ Adaptive sizing: {batch_config['adaptive_sizing']}")
        print(f"   â€¢ Target batch time: {batch_config['target_batch_time']}s")
        print(f"   â€¢ Batch size range: [{batch_config['min_batch_size']}, {batch_config['max_batch_size']}]")
        print(f"   â€¢ Load balancing: {batch_config['enable_load_balancing']}")
        print(f"   â€¢ Memory limit: {batch_config['memory_limit_mb']}MB")
        print(f"   â€¢ Priority strategy: {batch_config['priority_strategy']}")
        
        # Progress callback for optimized batch processing
        def optimized_progress_callback(processed, total, message):
            progress = (processed / total) * 100 if total > 0 else 0
            print(f"ğŸ“ˆ Progress: {progress:.1f}% ({processed}/{total}) - {message}")
        
        # Initialize analyzer for this demonstration
        analyzer = AIThreatAnalyzer()
        
        # Create mock servers for batch processing optimization demo
        demo_servers = [
            create_sample_mcp_server(),
            MCPServerInfo(
                server_id="enhanced-filesystem-server-67890",
                server_url="http://localhost:9090",
                tools=[
                    MCPTool(
                        name="advanced_file_operations",
                        description="Advanced file system operations with security features",
                        input_schema={
                            "type": "object",
                            "properties": {
                                "operation": {"type": "string", "description": "File operation type"},
                                "path": {"type": "string", "description": "Target path"}
                            },
                            "required": ["operation", "path"]
                        }
                    )
                ],
                metadata={"name": "Enhanced Filesystem Server", "version": "2.0"},
                capabilities=[],
                transport_type="https",
                host="localhost",
                port=9090
            )
        ]
        
        # Run optimized batch analysis
        optimized_result = analyzer.analyze_threats_batch_optimized(
            mcp_servers=demo_servers,
            progress_callback=optimized_progress_callback,
            **batch_config
        )
        
        end_time = time.time()
        
        # Report results
        print(f"\nâœ… Optimized Batch Analysis Complete!")
        print(f"â±ï¸  Total execution time: {end_time - start_time:.2f}s")
        print(f"ğŸ“Š Successful analyses: {len(optimized_result['analyses'])}")
        print(f"âŒ Errors: 0")  # Errors are handled internally
        
        # Show optimization metrics
        stats = optimized_result['statistics']
        opt_metrics = stats.get('optimization_metrics', {})
        
        print(f"\nğŸ¯ Optimization Performance:")
        print(f"   â€¢ Average batch size: {opt_metrics.get('avg_batch_size', 0):.1f}")
        print(f"   â€¢ Memory efficiency: {opt_metrics.get('memory_efficiency', 0):.2f} MB/tool")
        print(f"   â€¢ Total memory used: {opt_metrics.get('total_memory_used_mb', 0):.1f} MB")
        print(f"   â€¢ Processing rate: {stats.get('tools_per_second', 0):.1f} tools/second")
        print(f"   â€¢ Success rate: {stats.get('success_rate', 0)*100:.1f}%")
        
        # Show provider distribution if load balancing was used
        provider_dist = opt_metrics.get('provider_distribution', {})
        if provider_dist:
            print(f"\nâš–ï¸ Provider Load Distribution:")
            for provider, count in provider_dist.items():
                print(f"   â€¢ {provider}: {count} batches")
        
        # Show batch details
        batch_details = optimized_result.get('optimization_metrics', [])
        if batch_details:
            print(f"\nğŸ“‹ Batch Performance Details:")
            for i, batch in enumerate(batch_details[:3], 1):  # Show first 3 batches
                print(f"   Batch {i}: size={batch['batch_size']}, "
                     f"time={batch['batch_time']:.1f}s, "
                     f"memory={batch['memory_used_mb']:.1f}MB")
                if i >= 3 and len(batch_details) > 3:
                    print(f"   ... and {len(batch_details) - 3} more batches")
                    break
        
        print(f"\nğŸ† F4.3 Optimized Batch Processing Features Demonstrated:")
        print(f"   âœ… Adaptive batch sizing automatically adjusted based on performance")
        print(f"   âœ… Memory usage monitored and optimized per batch")
        print(f"   âœ… Load balancing distributed workload across providers")
        print(f"   âœ… Intelligent prioritization optimized processing order") 
        print(f"   âœ… Comprehensive performance metrics and analytics")
        print(f"   âœ… Production-ready optimization engine")
        
    except Exception as e:
        print(f"âŒ Error in optimized batch processing demonstration: {str(e)}")
        import traceback
        traceback.print_exc()

    print("\n" + "ğŸ‰" + "="*78)
    print("ğŸ‰ ALL DEMONSTRATIONS COMPLETE!")
    print("ğŸ‰" + "="*78)
    print("\nâœ¨ Summary of F4.2 + F4.3 Advanced Features Demonstrated:")
    print("   ğŸš€ Real-time result streaming as analyses complete (F4.2)")
    print("   ğŸ”¬ Stage-by-stage progress for individual tools (F4.2)")
    print("   ğŸ“¦ Batch streaming with resource management (F4.2)")
    print("   ğŸ“¡ Multiple event types for comprehensive monitoring (F4.2)")
    print("   âš¡ Zero wait time for first results (F4.2)")
    print("   ğŸ¯ Production-ready streaming architecture (F4.2)")
    print("   ğŸš€ Adaptive batch sizing for optimal performance (F4.3)")
    print("   âš–ï¸ Intelligent load balancing across providers (F4.3)")
    print("   ğŸ’¾ Memory usage optimization and monitoring (F4.3)")
    print("   ğŸ“ˆ Smart prioritization and scheduling (F4.3)")
    print("   ğŸ“Š Advanced performance analytics (F4.3)")
    print("\nğŸ† Phase 4 F4.2 + F4.3 Implementation: COMPLETE!")
    print("\n" + "="*78)
    print("The AI-powered threat analysis system now provides:")
    print("â€¢ Dynamic analysis of any MCP tool (not just hardcoded ones)")
    print("â€¢ Multiple AI provider support (OpenAI, Anthropic, Local LLM)")
    print("â€¢ Intelligent caching to reduce costs")
    print("â€¢ Comprehensive threat modeling and mitigation strategies")
    print("â€¢ Basic batch processing capabilities")
    print("â€¢ Parallel processing for improved performance (F4.1)")
    print("â€¢ ğŸ†• Real-time streaming analysis results (F4.2)")
    print("â€¢ ğŸ†• Stage-by-stage streaming for detailed progress (F4.2)")
    print("â€¢ ğŸ†• Batch streaming with resource management (F4.2)")
    print("â€¢ ğŸ†• Adaptive batch sizing for optimal performance (F4.3)")
    print("â€¢ ğŸ†• Intelligent load balancing across AI providers (F4.3)")
    print("â€¢ ğŸ†• Memory usage optimization and monitoring (F4.3)")
    print("â€¢ ğŸ†• Smart prioritization and scheduling (F4.3)")
    print("â€¢ ğŸ†• Advanced performance analytics (F4.3)")
    print("â€¢ Fallback to rule-based analysis when AI is unavailable")
    print("\nTo configure API keys, create a .env file or set environment variables:")
    print("AI_PROVIDER=anthropic")
    print("AI_ANTHROPIC_API_KEY=your_key_here")
    print("AI_OPENAI_API_KEY=your_key_here")


if __name__ == "__main__":
    main() 